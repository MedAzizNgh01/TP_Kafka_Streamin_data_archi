# ğŸš€ Streaming Data Pipeline

[![Kafka](https://img.shields.io/badge/Apache%20Kafka-000?style=for-the-badge&logo=apachekafka)](https://kafka.apache.org/)
[![MongoDB](https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=for-the-badge&logo=mongodb&logoColor=white)](https://www.mongodb.com/)
[![Postgres](https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)](https://www.python.org/)

Un pipeline de donnÃ©es en streaming complet utilisant Apache Kafka, MongoDB, PostgreSQL et Tableau.

## ğŸ“Œ Description

Ce projet implÃ©mente une architecture robuste pour l'ingestion, le traitement et la visualisation de donnÃ©es en temps rÃ©el. Il combine plusieurs technologies pour crÃ©er un pipeline de donnÃ©es complet, rÃ©silient et scalable.


## ğŸ› ï¸ Technologies

<div align="center">
<table>
  <tr>
    <td align="center" width="96">
      <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/apachekafka/apachekafka-original.svg" width="48" height="48" alt="Kafka" />
      <br>Apache Kafka
    </td>
    <td align="center" width="96">
      <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/apache/apache-original.svg" width="48" height="48" alt="Kafka Connect" />
      <br>Kafka Connect
    </td>
    <td align="center" width="96">
      <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/apache/apache-original.svg" width="48" height="48" alt="Flink" />
      <br>Apache Flink
    </td>
    <td align="center" width="96">
      <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/mongodb/mongodb-original.svg" width="48" height="48" alt="MongoDB" />
      <br>MongoDB
    </td>
  </tr>
  <tr>
    <td align="center" width="96">
      <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/postgresql/postgresql-original.svg" width="48" height="48" alt="PostgreSQL" />
      <br>PostgreSQL
    </td>
    <td align="center" width="96">
      <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" width="48" height="48" alt="Python" />
      <br>Python
    </td>
    <td align="center" width="96">
      <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/docker/docker-original.svg" width="48" height="48" alt="Docker" />
      <br>Docker
    </td>
    <td align="center" width="96">
      <img src="https://cdn.worldvectorlogo.com/logos/tableau-software.svg" width="48" height="48" alt="Tableau" />
      <br>Tableau
    </td>
  </tr>
</table>
</div>
## ğŸ”§ Installation

```bash
# Cloner le repository
git clone https://github.com/your-username/streaming-data-pipeline.git
cd streaming-data-pipeline

# DÃ©marrer les services avec Docker Compose
docker-compose up -d
```

## ğŸš€ Utilisation

### Exercice 1: Ingestion CSV vers Kafka

```bash
# DÃ©marrer le producteur Kafka
python producer.py

# DÃ©marrer le consommateur Kafka
python consumer.py
```

### Exercice 2: Traitement par fenÃªtres

```bash
# DÃ©marrer le producteur rÃ©silient
python producer_resilient.py

# DÃ©marrer le consommateur rÃ©silient
python consumer_resilient.py

# DÃ©marrer le traitement par fenÃªtres
python kafka_monitor.py
```

### Exercice 3: Pipeline complet

```bash
# Ã‰tape 1: GÃ©nÃ©ration de donnÃ©es
python producer.py

# Ã‰tape 2: MongoDB Consumer
python consumer.py

# Ã‰tape 3: Export CSV planifiÃ©
python schedule_export.py

# Ã‰tape 4: Migration vers PostgreSQL
python pg.py
```

## ğŸ“Š Visualisation

- Connectez Tableau Ã  la base de donnÃ©es PostgreSQL
- Importez les tableaux de bord fournis
- Explorez les donnÃ©es avec des filtres interactifs par pÃ©riode, rÃ©gion et produit

## ğŸ—ï¸ Structure du projet

<details>
<summary>ğŸ“‚ Cliquez pour voir l'arborescence du projet</summary>

```
streaming-data-pipeline/
â”œâ”€â”€ ğŸ“„ docker-compose.yml        # Configuration des conteneurs
â”œâ”€â”€ ğŸ“‚ exercice1/
â”‚   â”œâ”€â”€ ğŸ“„ connector.json        # Configuration Kafka Connect
â”‚   â”œâ”€â”€ ğŸ producer.py           # Producteur CSV vers Kafka
â”‚   â””â”€â”€ ğŸ consumer.py           # Consommateur Kafka
â”œâ”€â”€ ğŸ“‚ exercice2/
â”‚   â”œâ”€â”€ ğŸ producer_resilient.py # Producteur avec rÃ©silience
â”‚   â”œâ”€â”€ ğŸ consumer_resilient.py # Consommateur avec gestion d'offsets
â”‚   â””â”€â”€ ğŸ kafka_monitor.py      # Traitement par fenÃªtres
â””â”€â”€ ğŸ“‚ exercice3/
    â”œâ”€â”€ ğŸ producer.py           # GÃ©nÃ©ration de donnÃ©es de ventes
    â”œâ”€â”€ ğŸ consumer.py           # Stockage dans MongoDB
    â”œâ”€â”€ ğŸ schedule_export.py    # Export CSV rÃ©gulier
    â””â”€â”€ ğŸ pg.py                 # Migration vers PostgreSQL
```
</details>

## âš™ï¸ FonctionnalitÃ©s

<div align="center">

| FonctionnalitÃ© | Description |
| --- | --- |
| ğŸ“¥ **Ingestion de donnÃ©es en streaming** | Lecture continue de fichiers CSV |
| ğŸ”„ **RÃ©silience** | StratÃ©gies de reprise, rÃ©plication et gestion des pannes |
| ğŸ“ˆ **ScalabilitÃ©** | Architecture distribuÃ©e supportant la montÃ©e en charge |
| â±ï¸ **Traitement par fenÃªtres** | AgrÃ©gation par fenÃªtres temporelles |
| ğŸ—„ï¸ **Stockage multi-niveaux** | Pipeline complet de Kafka Ã  PostgreSQL |
| ğŸ“Š **Visualisation** | Tableaux de bord dynamiques avec Tableau |

</div>

## ğŸ“ DÃ©tails d'implÃ©mentation

<details>
<summary><b>ğŸ”„ Exercice 1: Ingestion CSV vers Kafka</b></summary>

```mermaid
graph LR
    A[Fichier CSV] -->|FileStreamSourceConnector| B[Topic Kafka]
    A -->|producer.py| B
    B -->|consumer.py| C[Console/Affichage]
    style A fill:#f9f9f9,stroke:#333,stroke-width:2px
    style B fill:#c5e8f7,stroke:#333,stroke-width:2px
    style C fill:#f9f9f9,stroke:#333,stroke-width:2px
```

- âœ… Utilisation de Kafka Connect avec FileStreamSourceConnector
- âœ… DÃ©veloppement d'un producteur Python pour la surveillance continue
- âœ… Transformation des donnÃ©es CSV en JSON
</details>

<details>
<summary><b>ğŸ”„ Exercice 2: Kafka RÃ©silient & Traitement par fenÃªtres</b></summary>

```mermaid
graph LR
    A[producer_resilient.py] -->|partitionnement| B[Cluster Kafka<br>3 brokers]
    B -->|gestion des offsets| C[consumer_resilient.py]
    B -->|fenÃªtres temporelles| D[kafka_monitor.py]
    style A fill:#f9f9f9,stroke:#333,stroke-width:2px
    style B fill:#c5e8f7,stroke:#333,stroke-width:2px
    style C fill:#f9f9f9,stroke:#333,stroke-width:2px
    style D fill:#f9f9f9,stroke:#333,stroke-width:2px
```

- âœ… Cluster Kafka avec 3 brokers pour la haute disponibilitÃ©
- âœ… Configuration acks=all, retries, backoff pour la rÃ©silience
- âœ… Traitement par fenÃªtres glissantes d'une minute
</details>

<details>
<summary><b>ğŸ”„ Exercice 3: Pipeline MongoDB-PostgreSQL</b></summary>

```mermaid
graph LR
    A[producer.py] -->|GÃ©nÃ©ration de donnÃ©es| B[Topic Kafka]
    B -->|consumer.py| C[MongoDB]
    C -->|schedule_export.py| D[Fichier CSV]
    D -->|pg.py| E[PostgreSQL]
    E -->|connexion| F[Tableau]
    style A fill:#f9f9f9,stroke:#333,stroke-width:2px
    style B fill:#c5e8f7,stroke:#333,stroke-width:2px
    style C fill:#c7e9c0,stroke:#333,stroke-width:2px
    style D fill:#f9f9f9,stroke:#333,stroke-width:2px
    style E fill:#dad5eb,stroke:#333,stroke-width:2px
    style F fill:#ffeaa5,stroke:#333,stroke-width:2px
```

- âœ… GÃ©nÃ©ration de donnÃ©es de ventes (produit, rÃ©gion, montant, date)
- âœ… Stockage dans MongoDB avec indexation
- âœ… Export CSV programmÃ© toutes les 2 minutes
- âœ… Migration vers PostgreSQL pour l'analyse avancÃ©e
- âœ… Visualisation avec Tableau
</details>

